# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: jk-esm2-training  
  namespace: tpu-namespace
  annotations:
    alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepool # 1:1 job replica to node pool assignment
spec:
  successPolicy:
    operator: All
    targetReplicatedJobs:
    - slice
  failurePolicy:
    maxRestarts: 1  
  replicatedJobs:
    - name: slice    # Part of the name of the child Jobs (<replicateJobName>)
      replicas: 1    # Number of slices
      template:
        spec:
          parallelism: 64   # Must be set to number of nodes in each node pool
          completions: 64   # Must be set to number of nodes in each node pool
          backoffLimit: 0   # Must be set to 0. Fail the job when any pod fails.
          template:
            metadata:
              #annotations:
              #  gke-gcsfuse/volumes: "true"
            spec:
              hostNetwork: false # Need to enable WID for GCS 
              serviceAccount: tpu-sa
              dnsPolicy: ClusterFirstWithHostNet
              nodeSelector:
                cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice
                cloud.google.com/gke-tpu-topology: 16x16
              containers:
              - name: tpu-job 
                image: pytorch-xla
                ports:
                - containerPort: 8479
                - containerPort: 8478
                - containerPort: 8477
                - containerPort: 8476
                - containerPort: 8431 # Port to export TPU usage metrics, if supported
                securityContext:
                  privileged: true
                command:
                - bash
                - -c
                - |
                  echo Copying training data to a local folder 
                  #mkdir -p /model_data/datasets/uniref 
                  mkdir /datasets/uniref
                  #cp -r /gcs/gsa-pso-aiml-repository/uniref_data/tokenized/arrow/* /model_data/datasets/uniref
                  gcloud storage rsync --recursive --no-clobber  gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/ /datasets/uniref 
                  #ls -la /model_data/datasets/uniref 
                  ls -la /datasets/uniref
                  
                  echo -------- Starting training 
                   
                  #tail -f /dev/null
                  python run_esm2.py \
                  --model_id "facebook/esm2_t48_15B_UR50D" \
                  --dataset_dir /datasets/uniref \
                  --output_dir /runs/run1 \
                  --cache_dir /tmp \
                  --overwrite_output_dir \
                  --per_device_train_batch_size 1024 \
                  --do_train \
                  --save_strategy no \
                  --logging_strategy steps \
                  --logging_first_step \
                  --logging_steps 10 \
                  --dataloader_drop_last yes \
                  --fsdp "full_shard" \
                  --fsdp_config fsdp_config.json \
                  --max_steps 100 2>&1 | tee execution.log

                  if [[ $? -eq 0 ]]; then
                      echo "Training script succeeded"
                  else 
                      echo "Training script failed"
                      ls -la 
                  fi
                  #gsutil cp execution.log gs://gsa-pso-aiml-repository/ESM216B-256-tests/traces/
                env:
                - name: PJRT_DEVICE
                  value: 'TPU'
                - name: XLA_USE_BF16
                  value: '1'
                #- name: XLA_IR_DEBUG 
                #  value: '1'
                #- name: XLA_HLO_DEBUG
                #  value: '1'
                #- name: PT_XLA_DEBUG
                #  value: '1'
                #- name: 'XLA_SAVE_TENSORS_FILE'
                #  value: 'trace.ir'
                #- name: LIBTPU_INIT_ARGS
                #  value: '--xla_tpu_enable_data_parallel_all_reduce_opt=true --xla_tpu_data_parallel_opt_different_sized_ops=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_overlap_compute_collective_tc=true --xla_enable_async_all_gather=true'
                #volumeMounts:
                #- name: gcs-fuse-csi-vol
                #  mountPath: /gcs
                #  readOnly: false
                #- name: data-volume
                #  mountPath: /model_data
                resources:
                  limits:
                    google.com/tpu: 4 # Number of Cloud TPU VMs per worker
              #volumes:
              #- name: gcs-fuse-csi-vol
              #  csi:
              #    driver: gcsfuse.csi.storage.gke.io
              #    readOnly: false
              #    volumeAttributes:
              #      bucketName: _
              #      mountOptions: "implicit-dirs"
              #- name: data-volume
              #  ephemeral:
              #    volumeClaimTemplate:
              #      metadata:
              #        labels:
              #          type: pso-model-data
              #      spec:
              #        accessModes: [ "ReadWriteOnce" ]
              #        storageClassName: "premium-rwo"
              #        resources:
              #          requests:
              #            storage: 200Gi