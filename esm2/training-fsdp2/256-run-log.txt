Generating tags...
 - pytorch-xla -> us-central1-docker.pkg.dev/dmitriy-vertex-dev01/gsa-pso-gke-tpu-gpu-training/pytorch-xla:latest
Checking cache...
 - pytorch-xla: Found Remotely
Tags used in deployment:
 - pytorch-xla -> us-central1-docker.pkg.dev/dmitriy-vertex-dev01/gsa-pso-gke-tpu-gpu-training/pytorch-xla:latest@sha256:9781d27ca860fa3b98f64eb0899195e8bbfc5b7e6195d924ffce1736ab3ad673
Starting deploy...
 - jobset.jobset.x-k8s.io/jk-esm2-training created
Waiting for deployments to stabilize...
Deployments stabilized in 210.32577ms
Listing files to watch...
 - pytorch-xla
Press Ctrl+C to exit
Watching for changes...
[tpu-job]Copying training data to a local folder
[tpu-job]mkdir: cannot create directory ‘/datasets/uniref’: No such file or directory
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 136439946635072 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]Copying training data to a local folder
[tpu-job]mkdir: cannot create directory ‘/datasets/uniref’: No such file or directory
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 134865223743296 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]Copying training data to a local folder
[tpu-job]mkdir: cannot create directory ‘/datasets/uniref’: No such file or directory
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 135784187701056 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]Copying training data to a local folder
[tpu-job]mkdir: cannot create directory ‘/datasets/uniref’: No such file or directory
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 7 thread 132523498137408 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]Copying training data to a local folder
[tpu-job]mkdir: cannot create directory ‘/datasets/uniref’: No such file or directory
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 137622259304256 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 135993225963328 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 139996553639744 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 134056668657472 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 140156971722560 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 132328689534784 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 135528340903744 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 135136424372032 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 132029901096768 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 140078126081856 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 138585132742464 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 132580936853312 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 135992746211136 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 138407361685312 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 134650472777536 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 136201676224320 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 136377820157760 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 137806477338432 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 136463804680000 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 140488627058496 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 132481859114816 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 135273132222272 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job]At gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/**, worker process 8 thread 140672355309376 listed 71...
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/dataset_dict.json to file:///datasets/uniref/dataset_dict.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/data-00000-of-00001.arrow to file:///datasets/uniref/test/data-00000-of-00001.arrow
[tpu-job]  
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/dataset_info.json to file:///datasets/uniref/test/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/test/state.json to file:///datasets/uniref/test/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00000-of-00062.arrow to file:///datasets/uniref/train/data-00000-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00001-of-00062.arrow to file:///datasets/uniref/train/data-00001-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00002-of-00062.arrow to file:///datasets/uniref/train/data-00002-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00003-of-00062.arrow to file:///datasets/uniref/train/data-00003-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00004-of-00062.arrow to file:///datasets/uniref/train/data-00004-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00005-of-00062.arrow to file:///datasets/uniref/train/data-00005-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00006-of-00062.arrow to file:///datasets/uniref/train/data-00006-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00007-of-00062.arrow to file:///datasets/uniref/train/data-00007-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00008-of-00062.arrow to file:///datasets/uniref/train/data-00008-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00009-of-00062.arrow to file:///datasets/uniref/train/data-00009-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00010-of-00062.arrow to file:///datasets/uniref/train/data-00010-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00011-of-00062.arrow to file:///datasets/uniref/train/data-00011-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00012-of-00062.arrow to file:///datasets/uniref/train/data-00012-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00013-of-00062.arrow to file:///datasets/uniref/train/data-00013-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00014-of-00062.arrow to file:///datasets/uniref/train/data-00014-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00015-of-00062.arrow to file:///datasets/uniref/train/data-00015-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00016-of-00062.arrow to file:///datasets/uniref/train/data-00016-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00017-of-00062.arrow to file:///datasets/uniref/train/data-00017-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00018-of-00062.arrow to file:///datasets/uniref/train/data-00018-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00019-of-00062.arrow to file:///datasets/uniref/train/data-00019-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00020-of-00062.arrow to file:///datasets/uniref/train/data-00020-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00021-of-00062.arrow to file:///datasets/uniref/train/data-00021-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00022-of-00062.arrow to file:///datasets/uniref/train/data-00022-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00023-of-00062.arrow to file:///datasets/uniref/train/data-00023-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00024-of-00062.arrow to file:///datasets/uniref/train/data-00024-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00025-of-00062.arrow to file:///datasets/uniref/train/data-00025-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00026-of-00062.arrow to file:///datasets/uniref/train/data-00026-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00027-of-00062.arrow to file:///datasets/uniref/train/data-00027-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00028-of-00062.arrow to file:///datasets/uniref/train/data-00028-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00029-of-00062.arrow to file:///datasets/uniref/train/data-00029-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00030-of-00062.arrow to file:///datasets/uniref/train/data-00030-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00031-of-00062.arrow to file:///datasets/uniref/train/data-00031-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00032-of-00062.arrow to file:///datasets/uniref/train/data-00032-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00033-of-00062.arrow to file:///datasets/uniref/train/data-00033-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00034-of-00062.arrow to file:///datasets/uniref/train/data-00034-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00035-of-00062.arrow to file:///datasets/uniref/train/data-00035-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00036-of-00062.arrow to file:///datasets/uniref/train/data-00036-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00037-of-00062.arrow to file:///datasets/uniref/train/data-00037-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00038-of-00062.arrow to file:///datasets/uniref/train/data-00038-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00039-of-00062.arrow to file:///datasets/uniref/train/data-00039-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00040-of-00062.arrow to file:///datasets/uniref/train/data-00040-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00041-of-00062.arrow to file:///datasets/uniref/train/data-00041-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00042-of-00062.arrow to file:///datasets/uniref/train/data-00042-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00043-of-00062.arrow to file:///datasets/uniref/train/data-00043-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00044-of-00062.arrow to file:///datasets/uniref/train/data-00044-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00045-of-00062.arrow to file:///datasets/uniref/train/data-00045-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00046-of-00062.arrow to file:///datasets/uniref/train/data-00046-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00047-of-00062.arrow to file:///datasets/uniref/train/data-00047-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00048-of-00062.arrow to file:///datasets/uniref/train/data-00048-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00049-of-00062.arrow to file:///datasets/uniref/train/data-00049-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00050-of-00062.arrow to file:///datasets/uniref/train/data-00050-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00051-of-00062.arrow to file:///datasets/uniref/train/data-00051-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00052-of-00062.arrow to file:///datasets/uniref/train/data-00052-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00053-of-00062.arrow to file:///datasets/uniref/train/data-00053-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00054-of-00062.arrow to file:///datasets/uniref/train/data-00054-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00055-of-00062.arrow to file:///datasets/uniref/train/data-00055-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00056-of-00062.arrow to file:///datasets/uniref/train/data-00056-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00057-of-00062.arrow to file:///datasets/uniref/train/data-00057-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00058-of-00062.arrow to file:///datasets/uniref/train/data-00058-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00059-of-00062.arrow to file:///datasets/uniref/train/data-00059-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00060-of-00062.arrow to file:///datasets/uniref/train/data-00060-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/data-00061-of-00062.arrow to file:///datasets/uniref/train/data-00061-of-00062.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/dataset_info.json to file:///datasets/uniref/train/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/train/state.json to file:///datasets/uniref/train/state.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/data-00000-of-00001.arrow to file:///datasets/uniref/validation/data-00000-of-00001.arrow
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/dataset_info.json to file:///datasets/uniref/validation/dataset_info.json
[tpu-job]Copying gs://gsa-pso-aiml-repository/uniref_data/tokenized/arrow/validation/state.json to file:///datasets/uniref/validation/state.json
[tpu-job].......................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 506.0MiB/s
[tpu-job].............................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 496.8MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job].............................................................................................................................................................................................................................................................................................................
[tpu-job]..............................................................................................................................................................................................................................................................................................................
[tpu-job].............................................................................................................................................................................................................................................................................................................
[tpu-job].............................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 499.2MiB/s
[tpu-job]..............................................................................................................................................................................................................................................................................................................
[tpu-job].............................................................................................................................................................................................................................................................................................................
[tpu-job]..............................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 498.5MiB/s
[tpu-job]
[tpu-job]Average throughput: 496.4MiB/s
[tpu-job]..............................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 499.3MiB/s
[tpu-job]...........................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 496.6MiB/s
[tpu-job]
[tpu-job]Average throughput: 496.6MiB/s
[tpu-job]
[tpu-job]Average throughput: 499.0MiB/s
[tpu-job]
[tpu-job]Average throughput: 496.7MiB/s
[tpu-job]
[tpu-job]Average throughput: 499.7MiB/s
[tpu-job]................................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]................................................................................................................................................................................................................................................................................................................
[tpu-job]...............................................................................................................................................................................................................................................................................................................
[tpu-job].............................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 491.9MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 495.3MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job].................................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 491.6MiB/s
[tpu-job]
[tpu-job]Average throughput: 498.4MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 490.2MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]..................................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]...........................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 489.6MiB/s
[tpu-job]
[tpu-job]Average throughput: 501.4MiB/s
[tpu-job]....................................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job].........................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 504.0MiB/s
[tpu-job].............................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 496.3MiB/s
[tpu-job].................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 486.2MiB/s
[tpu-job]........................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 490.6MiB/s
[tpu-job]...............................................................................................................................................................................................................................................................................................................
[tpu-job]......................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 492.5MiB/s
[tpu-job]
[tpu-job]Average throughput: 534.6MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 483.6MiB/s
[tpu-job]..............................................................................................................................................................................................................................................................................................................
[tpu-job]........................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 495.5MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job].......................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 481.3MiB/s
[tpu-job]
[tpu-job]Average throughput: 483.2MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]...................................................................................................................................................................................................................................................................................................................
[tpu-job]...................................................................................................................................................................................................................................................................................................................
[tpu-job].......................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 487.6MiB/s
[tpu-job]...........................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 486.9MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 476.0MiB/s
[tpu-job].................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 490.3MiB/s
[tpu-job].................................................................................................................................................................................................................................................................................................................
[tpu-job].....................................................................................................................................................................................................................................................................................................................
[tpu-job]...............................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 481.6MiB/s
[tpu-job]
[tpu-job]Average throughput: 483.6MiB/s
[tpu-job]....................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 490.6MiB/s
[tpu-job]
[tpu-job]Average throughput: 520.5MiB/s
[tpu-job]....................................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 486.0MiB/s
[tpu-job]............................................................................................................................................................................................................................................................................................................................
[tpu-job]......................................................................................................................................................................................................................................................................................................................
[tpu-job]..........................................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]...........................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 486.8MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 477.4MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 475.2MiB/s
[tpu-job]....................................................................................................................................................................................................................................................................................................................
[tpu-job]..................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 483.2MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 476.1MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 487.5MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job].................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 515.5MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job].......................................................................................................................................................................................................................................................................................................................
[tpu-job].....................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 480.6MiB/s
[tpu-job]..............................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 483.8MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]...................................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 517.3MiB/s
[tpu-job]
[tpu-job]Average throughput: 486.3MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]...........................................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]..............................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 472.3MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job].....................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 508.6MiB/s
[tpu-job]
[tpu-job]Average throughput: 474.7MiB/s
[tpu-job].................................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 471.2MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 467.7MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]...................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job].....................................................................................................................................................................................................................................................................................................
[tpu-job]..................................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 514.8MiB/s
[tpu-job]
[tpu-job]Average throughput: 465.1MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 509.6MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]...........................................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]..............................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 475.9MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 471.7MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job].....................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 510.8MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]..................................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 464.0MiB/s
[tpu-job]............................................................................................................................................................................................................................................................................................................................................
[tpu-job]..................................................................................................................................................................................................................................................................................................................................
[tpu-job]
[tpu-job]Average throughput: 451.1MiB/s
[tpu-job]
[tpu-job]Average throughput: 463.7MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job].........................................................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 454.5MiB/s
[tpu-job]......................................................................................................................................................................................................................................................................................................................................
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job]
[tpu-job]Average throughput: 459.8MiB/s
[tpu-job]total 24
[tpu-job]drwxr-xr-x 5 root root 4096 Apr 22 19:18 .
[tpu-job]drwxr-xr-x 3 root root 4096 Apr 22 19:18 ..
[tpu-job]-rw-r--r-- 1 root root   43 Mar 27 04:05 dataset_dict.json
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 test
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:19 train
[tpu-job]drwxr-xr-x 2 root root 4096 Apr 22 19:18 validation
[tpu-job]-------- Starting training
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,375 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,375 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,375 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,375 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,375 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,394 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,394 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,394 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,394 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,394 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,426 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,427 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,427 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,427 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,427 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,459 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,459 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,459 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,459 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,459 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,478 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,478 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,478 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,478 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,479 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,492 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,492 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,492 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,492 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,492 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,494 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,494 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,507 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,508 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,508 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,508 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,508 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,525 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,525 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,525 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,525 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,525 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,528 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,529 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,529 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,529 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,529 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,529 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,529 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,529 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,529 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,529 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,530 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,530 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,530 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,530 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,530 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,537 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,537 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,537 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,537 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,537 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,546 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,546 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,570 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,570 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,570 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,570 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,570 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,570 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,570 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,570 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,570 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,570 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,571 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,572 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,589 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,589 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,589 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,589 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,589 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,590 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,590 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,590 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,590 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,590 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,597 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,597 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,597 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,597 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,597 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,603 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,603 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,603 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,603 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,603 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,604 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,604 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,604 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,604 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,604 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,609 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,610 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,621 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,622 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,641 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,642 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,651 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,651 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,651 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,651 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,651 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,651 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,651 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,663 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,663 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,663 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,663 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,663 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,665 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,666 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,666 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,667 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,673 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,673 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,680 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,681 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,681 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,682 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,685 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,686 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,686 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,686 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,686 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,686 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,686 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,688 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,689 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,689 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,690 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,690 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,690 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,690 >> loading file tokenizer.json from cache at None
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,690 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,690 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,690 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,690 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,690 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,693 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,694 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,704 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,704 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,704 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,704 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,704 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,722 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,723 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,724 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,724 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,724 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,724 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,724 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,747 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,748 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,747 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,747 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,747 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,747 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,747 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,756 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,757 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,757 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,758 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,761 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,761 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,761 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,761 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,761 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,787 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,788 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,808 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,808 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,808 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,808 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,808 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,823 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,823 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,823 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,823 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,823 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,836 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,836 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,836 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,836 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,836 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,846 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,846 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,846 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,846 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,846 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,850 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,850 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,850 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,850 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,850 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,851 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,852 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,856 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,856 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,861 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,862 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,865 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,865 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,884 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,884 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,884 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,884 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,884 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,888 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,888 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,888 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,888 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,888 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,921 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,922 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,925 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,925 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,925 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,925 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,925 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,948 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,948 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,955 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,955 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,955 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,955 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,955 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,958 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,959 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,966 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,966 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,966 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,966 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,966 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,966 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,966 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,966 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,966 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,966 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,972 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,972 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,972 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,972 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,972 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,974 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,975 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,993 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,993 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,993 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,993 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,993 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,997 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,997 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,997 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:38,997 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,997 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:38,997 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:38,997 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,007 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,008 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,008 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,008 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,008 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,008 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,008 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,015 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,015 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,015 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,016 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,016 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,016 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,016 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,020 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,020 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,020 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,020 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,020 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,038 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,039 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,039 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,039 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,039 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,041 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,042 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,055 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,055 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,055 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,055 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,055 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,073 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,073 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,072 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,072 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,072 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,072 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,072 >> loading file tokenizer.json from cache at None
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,073 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,073 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,073 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,073 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,073 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,084 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,085 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,090 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,090 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,090 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,090 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,090 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,099 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,099 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,099 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,099 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,099 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,108 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,109 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,110 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,111 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,113 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,114 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,116 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,117 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,120 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,121 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,124 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,125 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,136 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,136 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,137 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,137 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,154 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,155 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,154 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,154 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,155 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,155 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,155 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,162 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,163 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,163 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,163 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,163 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,172 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,173 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,178 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,178 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,178 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,178 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,178 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,179 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,179 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,196 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,195 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,195 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,195 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,196 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,196 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,213 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,213 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,213 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,213 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,213 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,221 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,222 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,227 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,227 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,227 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,228 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,228 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,239 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,239 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,240 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,240 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,240 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,243 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,251 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,251 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,252 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,252 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,252 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,279 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,280 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,280 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,280 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,280 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,280 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,280 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,280 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,280 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,280 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,280 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,280 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,289 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,290 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,243 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,299 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,300 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,340 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,341 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,343 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,343 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,360 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,360 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,360 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,360 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,360 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,361 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,361 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,361 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,361 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,361 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,363 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,363 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,363 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,364 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,364 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,368 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/vocab.txt
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,368 >> loading file added_tokens.json from cache at None
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,368 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/special_tokens_map.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,368 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/tokenizer_config.json
[tpu-job][INFO|tokenization_utils_base.py:2087] 2024-04-22 19:19:39,368 >> loading file tokenizer.json from cache at None
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,388 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,388 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,423 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,424 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,453 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,454 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,490 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,491 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,534 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,534 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,600 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,601 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,679 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,679 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,695 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,696 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job][INFO|configuration_utils.py:726] 2024-04-22 19:19:39,802 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t48_15B_UR50D/snapshots/5fbca39631164edc1d402a5aa369f982f72ee282/config.json
[tpu-job][INFO|configuration_utils.py:789] 2024-04-22 19:19:39,803 >> Model config EsmConfig {
[tpu-job]  "_name_or_path": "facebook/esm2_t48_15B_UR50D",
[tpu-job]  "architectures": [
[tpu-job]    "EsmForMaskedLM"
[tpu-job]  ],
[tpu-job]  "attention_probs_dropout_prob": 0.0,
[tpu-job]  "classifier_dropout": null,
[tpu-job]  "emb_layer_norm_before": false,
[tpu-job]  "esmfold_config": null,
[tpu-job]  "hidden_act": "gelu",
[tpu-job]  "hidden_dropout_prob": 0.0,
[tpu-job]  "hidden_size": 5120,
[tpu-job]  "initializer_range": 0.02,
[tpu-job]  "intermediate_size": 20480,
[tpu-job]  "is_folding_model": false,
[tpu-job]  "layer_norm_eps": 1e-05,
[tpu-job]  "mask_token_id": 32,
[tpu-job]  "max_position_embeddings": 1026,
[tpu-job]  "model_type": "esm",
[tpu-job]  "num_attention_heads": 40,
[tpu-job]  "num_hidden_layers": 48,
[tpu-job]  "pad_token_id": 1,
[tpu-job]  "position_embedding_type": "rotary",
[tpu-job]  "token_dropout": true,
[tpu-job]  "torch_dtype": "bfloat16",
[tpu-job]  "transformers_version": "4.40.0.dev0",
[tpu-job]  "use_cache": true,
[tpu-job]  "vocab_list": null,
[tpu-job]  "vocab_size": 33
[tpu-job]}
[tpu-job]
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7994ab93fd10>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7dc713c2de90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7d09fbe93b90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7857eb934d10>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7b7270780250>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7f735fd60590>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7be889bc0a50>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7a8dd028f850>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7d4f621a7b90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7f5c0b013650>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x794303951e50>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7bb61f77cd10>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7dd34736aed0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7923d3c53850>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7aad01ee6250>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7bde8f7881d0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7837381fc350>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7b9d6a3c4c10>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x79665f5d4fd0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7f22f15ab890>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x79752b35af90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x79564b3d8e90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7bd06a79bb90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7860e6170310>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7e6555dd3f90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7e22297878d0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7f9303fb6590>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7ff9332301d0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x794178c9c2d0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7a372d3f8090>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7cf91df538d0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7904fe10f6d0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7b6eee276f10>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7ac578ee3250>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Loaded model: facebook/esm2_t48_15B_UR50D
[tpu-job]INFO:__main__:Model parameters: <bound method ModuleUtilsMixin.num_parameters of EsmForMaskedLM(
[tpu-job]  (esm): EsmModel(
[tpu-job]    (embeddings): EsmEmbeddings(
[tpu-job]      (word_embeddings): Embedding(33, 5120, padding_idx=1)
[tpu-job]      (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]      (position_embeddings): Embedding(1026, 5120, padding_idx=1)
[tpu-job]    )
[tpu-job]    (encoder): EsmEncoder(
[tpu-job]      (layer): ModuleList(
[tpu-job]        (0-47): 48 x EsmLayer(
[tpu-job]          (attention): EsmAttention(
[tpu-job]            (self): EsmSelfAttention(
[tpu-job]              (query): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (key): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (value): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]              (rotary_embeddings): RotaryEmbedding()
[tpu-job]            )
[tpu-job]            (output): EsmSelfOutput(
[tpu-job]              (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]              (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]            )
[tpu-job]            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]          )
[tpu-job]          (intermediate): EsmIntermediate(
[tpu-job]            (dense): Linear(in_features=5120, out_features=20480, bias=True)
[tpu-job]          )
[tpu-job]          (output): EsmOutput(
[tpu-job]            (dense): Linear(in_features=20480, out_features=5120, bias=True)
[tpu-job]            (dropout): Dropout(p=0.0, inplace=False)
[tpu-job]          )
[tpu-job]          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]        )
[tpu-job]      )
[tpu-job]      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    )
[tpu-job]    (contact_head): EsmContactPredictionHead(
[tpu-job]      (regression): Linear(in_features=1920, out_features=1, bias=True)
[tpu-job]      (activation): Sigmoid()
[tpu-job]    )
[tpu-job]  )
[tpu-job]  (lm_head): EsmLMHead(
[tpu-job]    (dense): Linear(in_features=5120, out_features=5120, bias=True)
[tpu-job]    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
[tpu-job]    (decoder): Linear(in_features=5120, out_features=33, bias=False)
[tpu-job]  )
[tpu-job])>
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7c65d46b8410>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7fb31eee2190>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x791540a45710>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7de8b20b57d0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x786691f67b90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7ab7e621a010>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7b56440c7a10>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7a1fb5497010>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x79b943a43b90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7cb2c8d6f390>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7f892fc4aa90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7d704f98aa90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x78aa49b70e90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x77fe6106f810>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7bf84de80350>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x780f8ae541d0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7ea13eef3890>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x792810f5fe50>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7c653ea855d0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7b94f6783410>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x798834e33050>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7d5e2b57f850>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7da49c4dd9d0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7f1b87077610>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7f9fc30b9e90>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7f4904a8d910>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x78ff65013b50>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7f61b76a4550>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x78e5436b0e50>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Logical mesh shape: OrderedDict([('fsdp', 256), ('tensor', 1)])
[tpu-job]INFO:__main__:Input sharding: ShardingSpec(mesh=<torch_xla.distributed.spmd.xla_sharding.Mesh object at 0x7fabfc438fd0>, partition_spec=('fsdp', None), minibatch=False, _tile_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _group_assignment=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _replication_groups=[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255]], _sharding_type=<ShardingType.PARTIAL: 5>)
[tpu-job]INFO:__main__:ESM2 classes to wrap: {<class 'transformers.models.esm.modeling_esm.EsmEmbeddings'>, <class 'transformers.models.esm.modeling_esm.EsmLMHead'>, <class 'transformers.models.esm.modeling_esm.EsmSelfOutput'>, <class 'transformers.models.esm.modeling_esm.EsmSelfAttention'>, <class 'transformers.models.esm.modeling_esm.EsmIntermediate'>, <class 'transformers.models.esm.modeling_esm.EsmOutput'>}
[tpu-job]INFO:__main__:Enabling gradient checkpointing
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]INFO:__main__:Starting training
[tpu-job]INFO:__main__:    Using FSDP
[tpu-job]INFO:__main__:    Start step: 1
[tpu-job]INFO:__main__:    Max step: 100
[tpu-job]INFO:__main__:    Global batch size: 1024
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1602: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.MaskedLMOutput'>
[tpu-job]  warnings.warn("For backward hooks to be called,"
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f7451f41ce1,7f7451f41d5f,7f55ed58a20f,7f55ed58af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 24 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7f7451f41ce1  (unknown)  raise
[tpu-job]    @     0x7f6efb2056c1        944  (unknown)
[tpu-job]    @     0x7f7451f41d60       3104  (unknown)
[tpu-job]    @     0x7f55ed58a210        248  (unknown)
[tpu-job]    @     0x7f55ed58af10  1640746008  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f7451f41ce1,7f6efb2056c0,7f7451f41d5f,7f55ed58a20f,7f55ed58af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:40.780240    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:40.780247    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:40.780249    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:40.780265    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:40.780269    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7d5248ec1ce1,7d5248ec1d5f,7d33dd58a20f,7d33dd58af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1207) on cpu 103 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7d5248ec1ce1  (unknown)  raise
[tpu-job]    @     0x7d4cef2056c1        944  (unknown)
[tpu-job]    @     0x7d5248ec1d60       3104  (unknown)
[tpu-job]    @     0x7d33dd58a210        248  (unknown)
[tpu-job]    @     0x7d33dd58af10  1757662232  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7d5248ec1ce1,7d4cef2056c0,7d5248ec1d5f,7d33dd58a20f,7d33dd58af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:41.148224    1207 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:41.148232    1207 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:41.148233    1207 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:41.148248    1207 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:41.148250    1207 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7bb744233ce1,7bb744233d5f,7b98e5d8b20f,7b98e5d8bf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 8 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7bb744233ce1  (unknown)  raise
[tpu-job]    @     0x7bb1eb2056c1        944  (unknown)
[tpu-job]    @     0x7bb744233d60       3104  (unknown)
[tpu-job]    @     0x7b98e5d8b210        248  (unknown)
[tpu-job]    @     0x7b98e5d8bf10  1534778392  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7bb744233ce1,7bb1eb2056c0,7bb744233d5f,7b98e5d8b20f,7b98e5d8bf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:41.403966    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:41.403974    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:41.403976    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:41.403992    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:41.403995    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=784c50a88ce1,784c50a88d5f,782dde58c20f,782dde58cf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1207) on cpu 56 from PID 158; stack trace: ***
[tpu-job]PC: @     0x784c50a88ce1  (unknown)  raise
[tpu-job]    @     0x7846fb2056c1        944  (unknown)
[tpu-job]    @     0x784c50a88d60       3104  (unknown)
[tpu-job]    @     0x782dde58c210        248  (unknown)
[tpu-job]    @     0x782dde58cf10  1870666776  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=784c50a88ce1,7846fb2056c0,784c50a88d5f,782dde58c20f,782dde58cf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:41.599890    1207 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:41.599897    1207 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:41.599899    1207 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:41.599923    1207 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:41.599934    1207 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7cfa4298dce1,7cfa4298dd5f,7ce4cd1b420f,7ce4cd1b4f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1207) on cpu 2 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7cfa4298dce1  (unknown)  raise
[tpu-job]    @     0x7cf4eb2056c1        944  (unknown)
[tpu-job]    @     0x7cfa4298dd60       3104  (unknown)
[tpu-job]    @     0x7ce4cd1b4210        248  (unknown)
[tpu-job]    @     0x7ce4cd1b4f10  1924000792  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7cfa4298dce1,7cf4eb2056c0,7cfa4298dd5f,7ce4cd1b420f,7ce4cd1b4f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:43.098102    1207 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:43.098110    1207 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:43.098111    1207 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:43.098127    1207 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:43.098129    1207 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7ffa82040ce1,7ffa82040d5f,7fe4fbffe20f,7fe4fbffef0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 84 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7ffa82040ce1  (unknown)  raise
[tpu-job]    @     0x7ff52f2056c1        944  (unknown)
[tpu-job]    @     0x7ffa82040d60       3104  (unknown)
[tpu-job]    @     0x7fe4fbffe210        248  (unknown)
[tpu-job]    @     0x7fe4fbffef10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7ffa82040ce1,7ff52f2056c0,7ffa82040d5f,7fe4fbffe20f,7fe4fbffef0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:43.146505    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:43.146513    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:43.146515    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:43.146530    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:43.146543    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7995d03d8ce1,7995d03d8d5f,79801751320f,798017513f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1209) on cpu 48 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7995d03d8ce1  (unknown)  raise
[tpu-job]    @     0x7990772056c1        944  (unknown)
[tpu-job]    @     0x7995d03d8d60       3104  (unknown)
[tpu-job]    @     0x798017513210        248  (unknown)
[tpu-job]    @     0x798017513f10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7995d03d8ce1,7990772056c0,7995d03d8d5f,79801751320f,798017513f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:43.268456    1209 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:43.268464    1209 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:43.268465    1209 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:43.268481    1209 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:43.268484    1209 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7dd4d1ecace1,7dd4d1ecad5f,7db6148a320f,7db6148a3f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1210) on cpu 0 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7dd4d1ecace1  (unknown)  raise
[tpu-job]    @     0x7dcf7b2056c1        944  (unknown)
[tpu-job]    @     0x7dd4d1ecad60       3104  (unknown)
[tpu-job]    @     0x7db6148a3210        248  (unknown)
[tpu-job]    @     0x7db6148a3f10  (unknown)  (unknown)
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7dc718bdbce1,7dc718bdbd5f,7da8c158a20f,7da8c158af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 40 from PID 158; stack trace: ***
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]PC: @     0x7dc718bdbce1  (unknown)  raise
[tpu-job]    @     0x7dc1bf2056c1        944  (unknown)
[tpu-job]    @     0x7dc718bdbd60       3104  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7dd4d1ecace1,7dcf7b2056c0,7dd4d1ecad5f,7db6148a320f,7db6148a3f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:43.805912    1210 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:43.805919    1210 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:43.805921    1210 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:43.805939    1210 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:43.805941    1210 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    @     0x7da8c158a210        248  (unknown)
[tpu-job]    @     0x7da8c158af10  1419078680  (unknown)
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7dc718bdbce1,7dc1bf2056c0,7dc718bdbd5f,7da8c158a20f,7da8c158af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:43.816741    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:43.816747    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:43.816750    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:43.816764    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:43.816767    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7d0b4893dce1,7d0b4893dd5f,7cecc158a20f,7cecc158af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 21 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7d0b4893dce1  (unknown)  raise
[tpu-job]    @     0x7d05ef2056c1        944  (unknown)
[tpu-job]    @     0x7d0b4893dd60       3104  (unknown)
[tpu-job]    @     0x7cecc158a210        248  (unknown)
[tpu-job]    @     0x7cecc158af10  (unknown)  (unknown)
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7862b4c96ce1,7862b4c96d5f,78451246c20f,78451246cf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 16 from PID 158; stack trace: ***
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7d0b4893dce1,7d05ef2056c0,7d0b4893dd5f,7cecc158a20f,7cecc158af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:43.847476    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:43.847483    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:43.847485    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:43.847502    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:43.847505    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]PC: @     0x7862b4c96ce1  (unknown)  raise
[tpu-job]    @     0x785d5b2056c1        944  (unknown)
[tpu-job]    @     0x7862b4c96d60       3104  (unknown)
[tpu-job]    @     0x78451246c210        248  (unknown)
[tpu-job]    @     0x78451246cf10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7862b4c96ce1,785d5b2056c0,7862b4c96d5f,78451246c20f,78451246cf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:43.860649    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:43.860656    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:43.860658    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:43.860674    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:43.860686    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7bb313d2dce1,7bb313d2dd5f,7b93ff0a820f,7b93ff0a8f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1207) on cpu 90 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7bb313d2dce1  (unknown)  raise
[tpu-job]    @     0x7badbf2056c1        944  (unknown)
[tpu-job]    @     0x7bb313d2dd60       3104  (unknown)
[tpu-job]    @     0x7b93ff0a8210        248  (unknown)
[tpu-job]    @     0x7b93ff0a8f10  301509656  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7bb313d2dce1,7badbf2056c0,7bb313d2dd5f,7b93ff0a820f,7b93ff0a8f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:43.985473    1207 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:43.985480    1207 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:43.985482    1207 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:43.985498    1207 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:43.985502    1207 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]    main()
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7e666ca88ce1,7e666ca88d5f,7e4805d8b20f,7e4805d8bf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1203) on cpu 26 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7e666ca88ce1  (unknown)  raise
[tpu-job]    @     0x7e611b2056c1        944  (unknown)
[tpu-job]    @     0x7e666ca88d60       3104  (unknown)
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]    @     0x7e4805d8b210        248  (unknown)
[tpu-job]    @     0x7e4805d8bf10  1677732888  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7e666ca88ce1,7e611b2056c0,7e666ca88d5f,7e4805d8b20f,7e4805d8bf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:44.120379    1203 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:44.120386    1203 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:44.120388    1203 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:44.120404    1203 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:44.120406    1203 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7858dd3dace1,7858dd3dad5f,783a61d8b20f,783a61d8bf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 93 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7858dd3dace1  (unknown)  raise
[tpu-job]    @     0x78538b2056c1        944  (unknown)
[tpu-job]    @     0x7858dd3dad60       3104  (unknown)
[tpu-job]    @     0x783a61d8b210        248  (unknown)
[tpu-job]    @     0x783a61d8bf10  2023050264  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7858dd3dace1,78538b2056c0,7858dd3dad5f,783a61d8b20f,783a61d8bf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:44.261499    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:44.261507    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:44.261509    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:44.261525    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:44.261530    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7be044420ce1,7be044420d5f,7bc25958a20f,7bc25958af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1209) on cpu 10 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7be044420ce1  (unknown)  raise
[tpu-job]    @     0x7bdaeb2056c1        944  (unknown)
[tpu-job]    @     0x7be044420d60       3104  (unknown)
[tpu-job]    @     0x7bc25958a210        248  (unknown)
[tpu-job]    @     0x7bc25958af10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7be044420ce1,7bdaeb2056c0,7be044420d5f,7bc25958a20f,7bc25958af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:44.353412    1209 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:44.353420    1209 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:44.353422    1209 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:44.353437    1209 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:44.353440    1209 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7be98c109ce1,7be98c109d5f,7bcb51d8b20f,7bcb51d8bf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 24 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7be98c109ce1  (unknown)  raise
[tpu-job]    @     0x7be4332056c1        944  (unknown)
[tpu-job]    @     0x7be98c109d60       3104  (unknown)
[tpu-job]    @     0x7bcb51d8b210        248  (unknown)
[tpu-job]    @     0x7bcb51d8bf10  929578008  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7be98c109ce1,7be4332056c0,7be98c109d5f,7bcb51d8b20f,7bcb51d8bf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:44.494458    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:44.494465    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:44.494467    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:44.494482    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:44.494484    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=798c8e046ce1,798c8e046d5f,796c10d8920f,796c10d89f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1366) on cpu 32 from PID 158; stack trace: ***
[tpu-job]PC: @     0x798c8e046ce1  (unknown)  raise
[tpu-job]    @     0x7987372056c1        944  (unknown)
[tpu-job]    @     0x798c8e046d60       3104  (unknown)
[tpu-job]    @     0x796c10d89210        248  (unknown)
[tpu-job]    @     0x796c10d89f10  2052860952  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=798c8e046ce1,7987372056c0,798c8e046d5f,796c10d8920f,796c10d89f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:44.552828    1366 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:44.552835    1366 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:44.552837    1366 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:44.552852    1366 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:44.552858    1366 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7aae2eb17ce1,7aae2eb17d5f,7a901158a20f,7a901158af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 41 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7aae2eb17ce1  (unknown)  raise
[tpu-job]    @     0x7aa8d72056c1        944  (unknown)
[tpu-job]    @     0x7aae2eb17d60       3104  (unknown)
[tpu-job]    @     0x7a901158a210        248  (unknown)
[tpu-job]    @     0x7a901158af10  445197336  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7aae2eb17ce1,7aa8d72056c0,7aae2eb17d5f,7a901158a20f,7a901158af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:44.697815    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:44.697823    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:44.697824    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:44.697840    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:44.697845    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]    results = trainer.train_loop()
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=795723ec9ce1,795723ec9d5f,7938c9d8b20f,7938c9d8bf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 34 from PID 158; stack trace: ***
[tpu-job]PC: @     0x795723ec9ce1  (unknown)  raise
[tpu-job]    @     0x7951cb2056c1        944  (unknown)
[tpu-job]    @     0x795723ec9d60       3104  (unknown)
[tpu-job]    @     0x7938c9d8b210        248  (unknown)
[tpu-job]    @     0x7938c9d8bf10  1464089624  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=795723ec9ce1,7951cb2056c0,795723ec9d5f,7938c9d8b20f,7938c9d8bf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:44.915474    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:44.915481    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:44.915483    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:44.915501    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:44.915503    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]    results = trainer.train_loop()
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7a4c47cc3ce1,7a4c47cc3d5f,7a2dfd58a20f,7a2dfd58af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1208) on cpu 72 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7a4c47cc3ce1  (unknown)  raise
[tpu-job]    @     0x7a46f32056c1        944  (unknown)
[tpu-job]    @     0x7a4c47cc3d60       3104  (unknown)
[tpu-job]    @     0x7a2dfd58a210        248  (unknown)
[tpu-job]    @     0x7a2dfd58af10  1201925144  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7a4c47cc3ce1,7a46f32056c0,7a4c47cc3d5f,7a2dfd58a20f,7a2dfd58af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:45.042958    1208 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:45.042966    1208 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:45.042968    1208 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:45.042983    1208 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:45.042986    1208 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]    main()
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7bd1a15c5ce1,7bd1a15c5d5f,7bb31d58a20f,7bb31d58af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 13 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7bd1a15c5ce1  (unknown)  raise
[tpu-job]    @     0x7bcc4b2056c1        944  (unknown)
[tpu-job]    @     0x7bd1a15c5d60       3104  (unknown)
[tpu-job]    @     0x7bb31d58a210        248  (unknown)
[tpu-job]    @     0x7bb31d58af10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7bd1a15c5ce1,7bcc4b2056c0,7bd1a15c5d5f,7bb31d58a20f,7bb31d58af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:45.089929    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:45.089937    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:45.089938    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:45.089963    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:45.089971    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7b73bfb8fce1,7b73bfb8fd5f,7b5e3a6cf20f,7b5e3a6cff0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 13 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7b73bfb8fce1  (unknown)  raise
[tpu-job]    @     0x7b6e6b2056c1        944  (unknown)
[tpu-job]    @     0x7b73bfb8fd60       3104  (unknown)
[tpu-job]    @     0x7b5e3a6cf210        248  (unknown)
[tpu-job]    @     0x7b5e3a6cff10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7b73bfb8fce1,7b6e6b2056c0,7b73bfb8fd5f,7b5e3a6cf20f,7b5e3a6cff0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:45.254810    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:45.254818    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:45.254820    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:45.254837    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:45.254839    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f24276cbce1,7f24276cbd5f,7f05f958a20f,7f05f958af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 84 from PID 158; stack trace: ***
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]PC: @     0x7f24276cbce1  (unknown)  raise
[tpu-job]    @     0x7f1ecf2056c1        944  (unknown)
[tpu-job]    @     0x7f24276cbd60       3104  (unknown)
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]    @     0x7f05f958a210        248  (unknown)
[tpu-job]    @     0x7f05f958af10  725904408  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f24276cbce1,7f1ecf2056c0,7f24276cbd5f,7f05f958a20f,7f05f958af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:45.451059    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:45.451066    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:45.451068    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:45.451089    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:45.451098    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=795771f2bce1,795771f2bd5f,79390e58c20f,79390e58cf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 24 from PID 158; stack trace: ***
[tpu-job]PC: @     0x795771f2bce1  (unknown)  raise
[tpu-job]    @     0x79521f2056c1        944  (unknown)
[tpu-job]    @     0x795771f2bd60       3104  (unknown)
[tpu-job]    @     0x79390e58c210        248  (unknown)
[tpu-job]    @     0x79390e58cf10  1623870488  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=795771f2bce1,79521f2056c0,795771f2bd5f,79390e58c20f,79390e58cf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:45.479994    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:45.480001    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:45.480003    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:45.480020    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:45.480029    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7967bc23cce1,7967bc23cd5f,79521ec5320f,79521ec53f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1215) on cpu 47 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7967bc23cce1  (unknown)  raise
[tpu-job]    @     0x7962632056c1        944  (unknown)
[tpu-job]    @     0x7967bc23cd60       3104  (unknown)
[tpu-job]    main()
[tpu-job]    @     0x79521ec53210        248  (unknown)
[tpu-job]    @     0x79521ec53f10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7967bc23cce1,7962632056c0,7967bc23cd5f,79521ec5320f,79521ec53f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:45.653077    1215 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:45.653084    1215 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:45.653086    1215 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:45.653101    1215 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:45.653103    1215 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7925411a6ce1,7925411a6d5f,7906ddd8b22f,7906ddd8bf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 21 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7925411a6ce1  (unknown)  raise
[tpu-job]    @     0x791feb2056c1        944  (unknown)
[tpu-job]    @     0x7925411a6d60       3072  (unknown)
[tpu-job]    @     0x7906ddd8b230        248  (unknown)
[tpu-job]    @     0x7906ddd8bf10  1618086904  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7925411a6ce1,791feb2056c0,7925411a6d5f,7906ddd8b22f,7906ddd8bf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:45.684419    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:45.684426    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:45.684429    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:45.684444    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:45.684446    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f5dea3b7ce1,7f5dea3b7d5f,7f3f5558a20f,7f3f5558af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1209) on cpu 57 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7f5dea3b7ce1  (unknown)  raise
[tpu-job]    @     0x7f58932056c1        944  (unknown)
[tpu-job]    @     0x7f5dea3b7d60       3104  (unknown)
[tpu-job]    @     0x7f3f5558a210        248  (unknown)
[tpu-job]    @     0x7f3f5558af10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f5dea3b7ce1,7f58932056c0,7f5dea3b7d5f,7f3f5558a20f,7f3f5558af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:45.742670    1209 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:45.742677    1209 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:45.742679    1209 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:45.742694    1209 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:45.742703    1209 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7a8f62ebbce1,7a8f62ebbd5f,7a70b30a820f,7a70b30a8f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 62 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7a8f62ebbce1  (unknown)  raise
[tpu-job]    @     0x7a8a0f2056c1        944  (unknown)
[tpu-job]    @     0x7a8f62ebbd60       3104  (unknown)
[tpu-job]    @     0x7a70b30a8210        248  (unknown)
[tpu-job]    @     0x7a70b30a8f10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7a8f62ebbce1,7a8a0f2056c0,7a8f62ebbd5f,7a70b30a820f,7a70b30a8f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:45.923958    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:45.923964    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:45.923966    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:45.923982    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:45.923993    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7943dbbd4ce1,7943dbbd4d5f,79259e58c20f,79259e58cf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 40 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7943dbbd4ce1  (unknown)  raise
[tpu-job]    @     0x793e872056c1        944  (unknown)
[tpu-job]    @     0x7943dbbd4d60       3104  (unknown)
[tpu-job]    @     0x79259e58c210        248  (unknown)
[tpu-job]    @     0x79259e58cf10  982834200  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7943dbbd4ce1,793e872056c0,7943dbbd4d5f,79259e58c20f,79259e58cf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:46.061668    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:46.061675    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:46.061677    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:46.061692    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:46.061695    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7e2500609ce1,7e2500609d5f,7e0638b1a20f,7e0638b1af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 106 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7e2500609ce1  (unknown)  raise
[tpu-job]    @     0x7e1fa72056c1        944  (unknown)
[tpu-job]    @     0x7e2500609d60       3104  (unknown)
[tpu-job]    @     0x7e0638b1a210        248  (unknown)
[tpu-job]    @     0x7e0638b1af10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7e2500609ce1,7e1fa72056c0,7e2500609d5f,7e0638b1a20f,7e0638b1af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:46.850353    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:46.850359    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:46.850361    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:46.850377    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:46.850381    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=79062b44ece1,79062b44ed5f,78e82958a20f,78e82958af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1207) on cpu 61 from PID 158; stack trace: ***
[tpu-job]PC: @     0x79062b44ece1  (unknown)  raise
[tpu-job]    @     0x7900d32056c1        944  (unknown)
[tpu-job]    @     0x79062b44ed60       3104  (unknown)
[tpu-job]    @     0x78e82958a210        248  (unknown)
[tpu-job]    @     0x78e82958af10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=79062b44ece1,7900d32056c0,79062b44ed5f,78e82958a20f,78e82958af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:47.657748    1207 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:47.657756    1207 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:47.657758    1207 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:47.657775    1207 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:47.657780    1207 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7c7b2a287ce1,7c7b2a287d5f,7c65a3ffe20f,7c65a3ffef0f&map= 
[tpu-job]*** SIGABRT received by PID 157 (TID 1210) on cpu 8 from PID 157; stack trace: ***
[tpu-job]PC: @     0x7c7b2a287ce1  (unknown)  raise
[tpu-job]    @     0x7c75d32056c1        944  (unknown)
[tpu-job]    @     0x7c7b2a287d60       3104  (unknown)
[tpu-job]    @     0x7c65a3ffe210        248  (unknown)
[tpu-job]    @     0x7c65a3ffef10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7c7b2a287ce1,7c75d32056c0,7c7b2a287d5f,7c65a3ffe20f,7c65a3ffef0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:47.997727    1210 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:47.997734    1210 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:47.997736    1210 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:47.997755    1210 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:47.997760    1210 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7adaaa402ce1,7adaaa402d5f,7abc25d8b20f,7abc25d8bf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1210) on cpu 95 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7adaaa402ce1  (unknown)  raise
[tpu-job]    @     0x7ad5532056c1        944  (unknown)
[tpu-job]    @     0x7adaaa402d60       3104  (unknown)
[tpu-job]    @     0x7abc25d8b210        248  (unknown)
[tpu-job]    @     0x7abc25d8bf10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7adaaa402ce1,7ad5532056c0,7adaaa402d5f,7abc25d8b20f,7abc25d8bf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:48.719414    1210 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:48.719422    1210 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:48.719424    1210 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:48.719440    1210 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:48.719453    1210 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f9468c4bce1,7f9468c4bd5f,7f7f436fd22f,7f7f436fdf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 32 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7f9468c4bce1  (unknown)  raise
[tpu-job]    @     0x7f8f0f2056c1        944  (unknown)
[tpu-job]    @     0x7f9468c4bd60       3072  (unknown)
[tpu-job]    @     0x7f7f436fd230        248  (unknown)
[tpu-job]    @     0x7f7f436fdf10  579156984  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f9468c4bce1,7f8f0f2056c0,7f9468c4bd5f,7f7f436fd22f,7f7f436fdf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:48.895271    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:48.895277    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:48.895279    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:48.895294    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:48.895297    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f9ee6dddce1,7f9ee6dddd5f,7f80b0d8920f,7f80b0d89f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 41 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7f9ee6dddce1  (unknown)  raise
[tpu-job]    @     0x7f998f2056c1        944  (unknown)
[tpu-job]    @     0x7f9ee6dddd60       3104  (unknown)
[tpu-job]    @     0x7f80b0d89210        248  (unknown)
[tpu-job]    @     0x7f80b0d89f10  859151384  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f9ee6dddce1,7f998f2056c0,7f9ee6dddd5f,7f80b0d8920f,7f80b0d89f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:48.915399    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:48.915407    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:48.915408    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:48.915424    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:48.915436    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7c663a70dce1,7c663a70dd5f,7c47bd58a20f,7c47bd58af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 103 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7c663a70dce1  (unknown)  raise
[tpu-job]    @     0x7c60e72056c1        944  (unknown)
[tpu-job]    @     0x7c663a70dd60       3104  (unknown)
[tpu-job]    @     0x7c47bd58a210        248  (unknown)
[tpu-job]    @     0x7c47bd58af10  2051574808  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7c663a70dce1,7c60e72056c0,7c663a70dd5f,7c47bd58a20f,7c47bd58af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:49.006712    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:49.006719    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:49.006721    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:49.006738    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:49.006743    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=78ab4c9c0ce1,78ab4c9c0d5f,788c91d8b20f,788c91d8bf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1207) on cpu 7 from PID 158; stack trace: ***
[tpu-job]PC: @     0x78ab4c9c0ce1  (unknown)  raise
[tpu-job]    @     0x78a5f72056c1        944  (unknown)
[tpu-job]    @     0x78ab4c9c0d60       3104  (unknown)
[tpu-job]    @     0x788c91d8b210        248  (unknown)
[tpu-job]    @     0x788c91d8bf10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=78ab4c9c0ce1,78a5f72056c0,78ab4c9c0d5f,788c91d8b20f,788c91d8bf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:49.985602    1207 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:49.985610    1207 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:49.985612    1207 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:49.985630    1207 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:49.985633    1207 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7de9bccd8ce1,7de9bccd8d5f,7dcb5cd8920f,7dcb5cd89f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1210) on cpu 46 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7de9bccd8ce1  (unknown)  raise
[tpu-job]    @     0x7de4672056c1        944  (unknown)
[tpu-job]    @     0x7de9bccd8d60       3104  (unknown)
[tpu-job]    @     0x7dcb5cd89210        248  (unknown)
[tpu-job]    @     0x7dcb5cd89f10  1562725400  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7de9bccd8ce1,7de4672056c0,7de9bccd8d5f,7dcb5cd8920f,7dcb5cd89f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:50.179199    1210 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:50.179207    1210 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:50.179209    1210 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:50.179224    1210 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:50.179227    1210 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=781404208ce1,781404208d5f,77fe62c5320f,77fe62c53f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 4 from PID 158; stack trace: ***
[tpu-job]PC: @     0x781404208ce1  (unknown)  raise
[tpu-job]    @     0x780eab2056c1        944  (unknown)
[tpu-job]    @     0x781404208d60       3104  (unknown)
[tpu-job]    @     0x77fe62c53210        248  (unknown)
[tpu-job]    @     0x77fe62c53f10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=781404208ce1,780eab2056c0,781404208d5f,77fe62c5320f,77fe62c53f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:50.300672    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:50.300680    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:50.300682    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:50.300700    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:50.300704    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f1c929aace1,7f1c929aad5f,7f07011d220f,7f07011d2f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1207) on cpu 48 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7f1c929aace1  (unknown)  raise
[tpu-job]    @     0x7f173f2056c1        944  (unknown)
[tpu-job]    @     0x7f1c929aad60       3104  (unknown)
[tpu-job]    @     0x7f07011d2210        248  (unknown)
[tpu-job]    @     0x7f07011d2f10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f1c929aace1,7f173f2056c0,7f1c929aad5f,7f07011d220f,7f07011d2f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:50.358854    1207 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:50.358861    1207 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:50.358863    1207 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:50.358879    1207 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:50.358884    1207 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7cc86e5a9ce1,7cc86e5a9d5f,7cb2b3ffc20f,7cb2b3ffcf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 88 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7cc86e5a9ce1  (unknown)  raise
[tpu-job]    @     0x7cc3172056c1        944  (unknown)
[tpu-job]    @     0x7cc86e5a9d60       3104  (unknown)
[tpu-job]    @     0x7cb2b3ffc210        248  (unknown)
[tpu-job]    @     0x7cb2b3ffcf10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7cc86e5a9ce1,7cc3172056c0,7cc86e5a9d5f,7cb2b3ffc20f,7cb2b3ffcf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:50.434387    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:50.434394    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:50.434396    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:50.434411    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:50.434414    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    main()
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7b6bf9e8bce1,7b6bf9e8bd5f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 100 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7b6bf9e8bce1  (unknown)  raise
[tpu-job]    @     0x7b66a72056c1        944  (unknown)
[tpu-job]    @     0x7b6bf9e8bd60  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7b6bf9e8bce1,7b66a72056c0,7b6bf9e8bd5f&map= 
[tpu-job]E0422 19:25:50.578208    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:50.578216    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:50.578218    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:50.578232    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:50.578241    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=792902e6fce1,792902e6fd5f,790a9d58a12f,790a9d58af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 68 from PID 158; stack trace: ***
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]PC: @     0x792902e6fce1  (unknown)  raise
[tpu-job]    @     0x7923af2056c1        944  (unknown)
[tpu-job]    @     0x792902e6fd60       3072  (unknown)
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]    @     0x790a9d58a130        248  (unknown)
[tpu-job]    @     0x790a9d58af10  1656663288  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=792902e6fce1,7923af2056c0,792902e6fd5f,790a9d58a12f,790a9d58af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:50.601584    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:50.601592    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:50.601594    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:50.601610    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:50.601620    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7b848c21dce1,7b848c21dd5f,7b663658c20f,7b663658cf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 102 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7b848c21dce1  (unknown)  raise
[tpu-job]    @     0x7b7f332056c1        944  (unknown)
[tpu-job]    @     0x7b848c21dd60       3104  (unknown)
[tpu-job]    @     0x7b663658c210        248  (unknown)
[tpu-job]    main()
[tpu-job]    @     0x7b663658cf10  1392077848  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7b848c21dce1,7b7f332056c0,7b848c21dd5f,7b663658c20f,7b663658cf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:50.632322    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:50.632329    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:50.632331    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:50.632348    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:50.632350    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f49fe6b6ce1,7f49fe6b6d5f,7f34bd83120f,7f34bd831f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1209) on cpu 74 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7f49fe6b6ce1  (unknown)  raise
[tpu-job]    @     0x7f44a72056c1        944  (unknown)
[tpu-job]    @     0x7f49fe6b6d60       3104  (unknown)
[tpu-job]    @     0x7f34bd831210        248  (unknown)
[tpu-job]    @     0x7f34bd831f10  1041804312  (unknown)
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=78108dc6cce1,78108dc6cd5f,77f218d8920f,77f218d89f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1210) on cpu 111 from PID 158; stack trace: ***
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f49fe6b6ce1,7f44a72056c0,7f49fe6b6d5f,7f34bd83120f,7f34bd831f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:50.762544    1209 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:50.762553    1209 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:50.762556    1209 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:50.762578    1209 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:50.762587    1209 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]PC: @     0x78108dc6cce1  (unknown)  raise
[tpu-job]    @     0x780b372056c1        944  (unknown)
[tpu-job]    @     0x78108dc6cd60       3104  (unknown)
[tpu-job]    @     0x77f218d89210        248  (unknown)
[tpu-job]    @     0x77f218d89f10  1914604568  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=78108dc6cce1,780b372056c0,78108dc6cd5f,77f218d8920f,77f218d89f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:50.777285    1210 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:50.777292    1210 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:50.777294    1210 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:50.777311    1210 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:50.777316    1210 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=79baac565ce1,79baac565d5f,799bda58c20f,799bda58cf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1207) on cpu 80 from PID 158; stack trace: ***
[tpu-job]PC: @     0x79baac565ce1  (unknown)  raise
[tpu-job]    @     0x79b5572056c1        944  (unknown)
[tpu-job]    @     0x79baac565d60       3104  (unknown)
[tpu-job]    @     0x799bda58c210        248  (unknown)
[tpu-job]    @     0x799bda58cf10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=79baac565ce1,79b5572056c0,79baac565d5f,799bda58c20f,799bda58cf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:50.931019    1207 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:50.931027    1207 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:50.931029    1207 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:50.931045    1207 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:50.931049    1207 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7867e09d9ce1,7867e09d9d5f,7849b8d8920f,7849b8d89f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1207) on cpu 92 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7867e09d9ce1  (unknown)  raise
[tpu-job]    @     0x78628b2056c1        944  (unknown)
[tpu-job]    @     0x7867e09d9d60       3104  (unknown)
[tpu-job]    @     0x7849b8d89210        248  (unknown)
[tpu-job]    @     0x7849b8d89f10  620059672  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7867e09d9ce1,78628b2056c0,7867e09d9d5f,7849b8d8920f,7849b8d89f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:51.193020    1207 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:51.193027    1207 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:51.193029    1207 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:51.193044    1207 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:51.193047    1207 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]    main()
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=799dba05ece1,799dba05ed5f,79801a46c20f,79801a46cf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1209) on cpu 82 from PID 158; stack trace: ***
[tpu-job]PC: @     0x799dba05ece1  (unknown)  raise
[tpu-job]    @     0x7998632056c1        944  (unknown)
[tpu-job]    @     0x799dba05ed60       3104  (unknown)
[tpu-job]    @     0x79801a46c210        248  (unknown)
[tpu-job]    @     0x79801a46cf10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=799dba05ece1,7998632056c0,799dba05ed5f,79801a46c20f,79801a46cf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:51.395138    1209 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:51.395146    1209 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:51.395148    1209 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:51.395164    1209 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:51.395170    1209 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7a355a9d2ce1,7a355a9d2d5f,7a17b246c20f,7a17b246cf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 27 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7a355a9d2ce1  (unknown)  raise
[tpu-job]    @     0x7a30072056c1        944  (unknown)
[tpu-job]    @     0x7a355a9d2d60       3104  (unknown)
[tpu-job]    @     0x7a17b246c210        248  (unknown)
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7ea26ba13ce1,7ea26ba13d5f,7e837a0a620f,7e837a0a6f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1207) on cpu 14 from PID 158; stack trace: ***
[tpu-job]    results = trainer.train_loop()
[tpu-job]    @     0x7a17b246cf10  (unknown)  (unknown)
[tpu-job]PC: @     0x7ea26ba13ce1  (unknown)  raise
[tpu-job]    @     0x7e9d132056c1        944  (unknown)
[tpu-job]    @     0x7ea26ba13d60       3104  (unknown)
[tpu-job]    @     0x7e837a0a6210        248  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7a355a9d2ce1,7a30072056c0,7a355a9d2d5f,7a17b246c20f,7a17b246cf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:51.470291    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:51.470299    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:51.470301    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:51.470316    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:51.470327    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    @     0x7e837a0a6f10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7ea26ba13ce1,7e9d132056c0,7ea26ba13d5f,7e837a0a620f,7e837a0a6f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:51.479310    1207 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:51.479317    1207 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:51.479319    1207 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:51.479335    1207 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:51.479338    1207 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=791545965ce1,791545965d5f,78f6a658c20f,78f6a658cf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1209) on cpu 11 from PID 158; stack trace: ***
[tpu-job]PC: @     0x791545965ce1  (unknown)  raise
[tpu-job]    @     0x790fef2056c1        944  (unknown)
[tpu-job]    @     0x791545965d60       3104  (unknown)
[tpu-job]    @     0x78f6a658c210        248  (unknown)
[tpu-job]    @     0x78f6a658cf10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=791545965ce1,790fef2056c0,791545965d5f,78f6a658c20f,78f6a658cf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:51.571011    1209 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:51.571018    1209 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:51.571020    1209 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:51.571053    1209 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:51.571057    1209 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7baa93dcace1,7baa93dcad5f,7b8d0fede20f,7b8d0fedef0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 8 from PID 158; stack trace: ***
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]PC: @     0x7baa93dcace1  (unknown)  raise
[tpu-job]    @     0x7ba53f2056c1        944  (unknown)
[tpu-job]    @     0x7baa93dcad60       3104  (unknown)
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]    @     0x7b8d0fede210        248  (unknown)
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]    @     0x7b8d0fedef10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7baa93dcace1,7ba53f2056c0,7baa93dcad5f,7b8d0fede20f,7b8d0fedef0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:51.841131    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:51.841138    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:51.841140    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:51.841156    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:51.841159    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7d5f36203ce1,7d5f36203d5f,7d4973ffc20f,7d4973ffcf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1207) on cpu 65 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7d5f36203ce1  (unknown)  raise
[tpu-job]    @     0x7d59e32056c1        944  (unknown)
[tpu-job]    @     0x7d5f36203d60       3104  (unknown)
[tpu-job]    @     0x7d4973ffc210        248  (unknown)
[tpu-job]    @     0x7d4973ffcf10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7d5f36203ce1,7d59e32056c0,7d5f36203d5f,7d4973ffc20f,7d4973ffcf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:51.872373    1207 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:51.872381    1207 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:51.872384    1207 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:51.872405    1207 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:51.872410    1207 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]    main()
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7bf93fe09ce1,7bf93fe09d5f,7bda7958a20f,7bda7958af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 16 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7bf93fe09ce1  (unknown)  raise
[tpu-job]    @     0x7bf3eb2056c1        944  (unknown)
[tpu-job]    @     0x7bf93fe09d60       3104  (unknown)
[tpu-job]    @     0x7bda7958a210        248  (unknown)
[tpu-job]    @     0x7bda7958af10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7bf93fe09ce1,7bf3eb2056c0,7bf93fe09d5f,7bda7958a20f,7bda7958af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:52.107529    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:52.107537    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:52.107539    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:52.107555    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:52.107558    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7ab8e8f16ce1,7ab8e8f16d5f,7a9a698a520f,7a9a698a5f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 16 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7ab8e8f16ce1  (unknown)  raise
[tpu-job]    @     0x7ab3932056c1        944  (unknown)
[tpu-job]    @     0x7ab8e8f16d60       3104  (unknown)
[tpu-job]    @     0x7a9a698a5210        248  (unknown)
[tpu-job]    @     0x7a9a698a5f10  2090298392  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7ab8e8f16ce1,7ab3932056c0,7ab8e8f16d5f,7a9a698a520f,7a9a698a5f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:52.181340    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:52.181347    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:52.181349    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:52.181365    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:52.181369    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7da62f255ce1,7da62f255d5f,7d87f558a20f,7d87f558af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 47 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7da62f255ce1  (unknown)  raise
[tpu-job]    @     0x7da0d72056c1        944  (unknown)
[tpu-job]    @     0x7da62f255d60       3104  (unknown)
[tpu-job]    @     0x7d87f558a210        248  (unknown)
[tpu-job]    @     0x7d87f558af10  922553368  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7da62f255ce1,7da0d72056c0,7da62f255d5f,7d87f558a20f,7d87f558af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:52.762404    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:52.762413    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:52.762415    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:52.762442    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:52.762449    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=78e64639ece1,78e64639ed5f,78c7e9d8b20f,78c7e9d8bf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 64 from PID 158; stack trace: ***
[tpu-job]PC: @     0x78e64639ece1  (unknown)  raise
[tpu-job]    @     0x78e0f32056c1        944  (unknown)
[tpu-job]    @     0x78e64639ed60       3104  (unknown)
[tpu-job]    @     0x78c7e9d8b210        248  (unknown)
[tpu-job]    @     0x78c7e9d8bf10  1502710808  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=78e64639ece1,78e0f32056c0,78e64639ed5f,78c7e9d8b20f,78c7e9d8bf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:52.947192    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:52.947200    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:52.947202    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:52.947217    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:52.947222    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7fb4882e8ce1,7fb4882e8d5f,7f961258c20f,7f961258cf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 40 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7fb4882e8ce1  (unknown)  raise
[tpu-job]    @     0x7faf332056c1        944  (unknown)
[tpu-job]    @     0x7fb4882e8d60       3104  (unknown)
[tpu-job]    @     0x7f961258c210        248  (unknown)
[tpu-job]    @     0x7f961258cf10  1929780248  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7fb4882e8ce1,7faf332056c0,7fb4882e8d5f,7f961258c20f,7f961258cf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:53.623246    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:53.623253    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:53.623255    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:53.623272    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:53.623275    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f648e6cace1,7f648e6cad5f,7f463558a20f,7f463558af0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 8 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7f648e6cace1  (unknown)  raise
[tpu-job]    @     0x7f5f372056c1        944  (unknown)
[tpu-job]    @     0x7f648e6cad60       3104  (unknown)
[tpu-job]    @     0x7f463558a210        248  (unknown)
[tpu-job]    @     0x7f463558af10  1447320600  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f648e6cace1,7f5f372056c0,7f648e6cad5f,7f463558a20f,7f463558af0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:54.411479    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:54.411486    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:54.411488    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:54.411504    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:54.411506    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7d71966ecce1,7d71966ecd5f,7d533e58c20f,7d533e58cf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 76 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7d71966ecce1  (unknown)  raise
[tpu-job]    @     0x7d6c3f2056c1        944  (unknown)
[tpu-job]    @     0x7d71966ecd60       3104  (unknown)
[tpu-job]    @     0x7d533e58c210        248  (unknown)
[tpu-job]    @     0x7d533e58cf10  1430674456  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7d71966ecce1,7d6c3f2056c0,7d71966ecd5f,7d533e58c20f,7d533e58cf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:54.874623    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:54.874630    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:54.874632    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:54.874648    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:54.874652    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f9fc8132ce1,7f9fc8132d5f,7f80928a720f,7f80928a7f0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 4 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7f9fc8132ce1  (unknown)  raise
[tpu-job]    @     0x7f9a732056c1        944  (unknown)
[tpu-job]    @     0x7f9fc8132d60       3104  (unknown)
[tpu-job]    @     0x7f80928a7210        248  (unknown)
[tpu-job]    @     0x7f80928a7f10  850988056  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7f9fc8132ce1,7f9a732056c0,7f9fc8132d5f,7f80928a720f,7f80928a7f0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:55.205256    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:55.205263    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:55.205265    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:55.205285    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:55.205296    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=7fad0729cce1,7fad0729cd5f,7f8e85d8b20f,7f8e85d8bf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 77 from PID 158; stack trace: ***
[tpu-job]PC: @     0x7fad0729cce1  (unknown)  raise
[tpu-job]    @     0x7fa7af2056c1        944  (unknown)
[tpu-job]    @     0x7fad0729cd60       3104  (unknown)
[tpu-job]    @     0x7f8e85d8b210        248  (unknown)
[tpu-job]    @     0x7f8e85d8bf10  2122411032  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=7fad0729cce1,7fa7af2056c0,7fad0729cd5f,7f8e85d8b20f,7f8e85d8bf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:25:57.500312    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:25:57.500320    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:25:57.500322    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:25:57.500338    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:25:57.500341    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]E0422 19:26:00.698710    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:01.036860    1207 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:01.295431    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:01.580126    1207 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Traceback (most recent call last):
[tpu-job]  File "/transformers/run_esm2.py", line 460, in <module>
[tpu-job]    main()
[tpu-job]  File "/transformers/run_esm2.py", line 449, in main
[tpu-job]    results = trainer.train_loop()
[tpu-job]              ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/transformers/run_esm2.py", line 334, in train_loop
[tpu-job]    batch = next(train_iterator)
[tpu-job]            ^^^^^^^^^^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 32, in __next__
[tpu-job]    return self.next()
[tpu-job]           ^^^^^^^^^^^
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/distributed/parallel_loader.py", line 44, in next
[tpu-job]    xm.mark_step()
[tpu-job]  File "/usr/local/lib/python3.11/site-packages/torch_xla/core/xla_model.py", line 1055, in mark_step
[tpu-job]    torch_xla._XLAC._xla_step_marker(
[tpu-job]RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 16.61G of 15.75G hbm. Exceeded hbm capacity by 886.69M.
[tpu-job]
[tpu-job]Total hbm usage >= 16.87G:
[tpu-job]    reserved        258.00M 
[tpu-job]    program          16.61G 
[tpu-job]    arguments            0B 
[tpu-job]
[tpu-job]Output size 0B; shares 0B with arguments.
[tpu-job]
[tpu-job]Program hbm requirement 16.61G:
[tpu-job]    global           21.54M
[tpu-job]    scoped           577.0K
[tpu-job]    HLO temp         16.59G (99.7% utilization: Unpadded (13.83G) Padded (13.87G), 16.4% fragmentation (2.72G))
[tpu-job]
[tpu-job]  Largest program allocations in hbm:
[tpu-job]
[tpu-job]  1. Size: 240.00M
[tpu-job]     Shape: bf16[256,20,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     Extra memory due to padding: 40.00M (1.2x expansion)
[tpu-job]     XLA label: fusion.19023 = fusion(bitcast.1010), kind=kCustom, output_to_operand_aliasing={{0}: (0, {})}, calls=fused_computation.11742
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  2. Size: 200.00M
[tpu-job]     Shape: bf16[20480,5120]{1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 200.00M
[tpu-job]     XLA label: all-gather.13118.remat = all-gather(copy-done.1123), channel_id=2029, replica_groups={{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  3. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16037 = fusion(get-tuple-element.26654, get-tuple-element.26655, get-tuple-element.26656, get-tuple-element.26657, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  4. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16081 = fusion(get-tuple-element.26762, get-tuple-element.26763, get-tuple-element.26764, get-tuple-element.26765, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  5. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15597 = fusion(get-tuple-element.25655, get-tuple-element.25656, get-tuple-element.25657, get-tuple-element.25658, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  6. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17137 = fusion(get-tuple-element.29171, get-tuple-element.29172, get-tuple-element.29175, get-tuple-element.29176, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  7. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17225 = fusion(get-tuple-element.29370, get-tuple-element.29371, get-tuple-element.29372, get-tuple-element.29373, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  8. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15641 = fusion(get-tuple-element.25761, get-tuple-element.25762, get-tuple-element.25763, get-tuple-element.25764, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  9. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16433 = fusion(get-tuple-element.27569, get-tuple-element.27570, get-tuple-element.27571, get-tuple-element.27572, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  10. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17093 = fusion(get-tuple-element.29065, get-tuple-element.29066, get-tuple-element.29067, get-tuple-element.29068, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  11. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17049 = fusion(get-tuple-element.28973, get-tuple-element.28974, get-tuple-element.28975, get-tuple-element.28976, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  12. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15685 = fusion(get-tuple-element.25860, get-tuple-element.25861, get-tuple-element.25862, get-tuple-element.25863, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  13. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.17005 = fusion(get-tuple-element.28868, get-tuple-element.28869, get-tuple-element.28870, get-tuple-element.28874, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  14. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16961 = fusion(get-tuple-element.28760, get-tuple-element.28761, get-tuple-element.28762, get-tuple-element.28763, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  15. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16917 = fusion(get-tuple-element.28668, get-tuple-element.28669, get-tuple-element.28670, get-tuple-element.28671, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  16. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15729 = fusion(get-tuple-element.25952, get-tuple-element.25953, get-tuple-element.25954, get-tuple-element.25955, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  17. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16873 = fusion(get-tuple-element.28576, get-tuple-element.28577, get-tuple-element.28578, get-tuple-element.28579, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  18. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16829 = fusion(get-tuple-element.28465, get-tuple-element.28466, get-tuple-element.28467, get-tuple-element.28468, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  19. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.16785 = fusion(get-tuple-element.28363, get-tuple-element.28364, get-tuple-element.28365, get-tuple-element.28366, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]  20. Size: 80.00M
[tpu-job]     Shape: bf16[4,512,20480]{2,1,0:T(8,128)(2,1)}
[tpu-job]     Unpadded size: 80.00M
[tpu-job]     XLA label: fusion.15773 = fusion(get-tuple-element.26060, get-tuple-element.26061, get-tuple-element.26062, get-tuple-element.26063, ...(+7)), kind=kOutput, output_to_operand_aliasing={{1}: (0, {}), {2}: (1, {}), {3}: (1, {}), {4}: (2, {}), {5}: (3, {}), {6}: (4, {})...
[tpu-job]     Allocation type: HLO temp
[tpu-job]     ==========================
[tpu-job]
[tpu-job]
[tpu-job]terminate called without an active exception
[tpu-job]https://symbolize.stripped_domain/r/?trace=790089c1ace1,790089c1ad5f,78e1ca58c20f,78e1ca58cf0f&map= 
[tpu-job]*** SIGABRT received by PID 158 (TID 1211) on cpu 16 from PID 158; stack trace: ***
[tpu-job]PC: @     0x790089c1ace1  (unknown)  raise
[tpu-job]    @     0x78fb372056c1        944  (unknown)
[tpu-job]    @     0x790089c1ad60       3104  (unknown)
[tpu-job]    @     0x78e1ca58c210        248  (unknown)
[tpu-job]    @     0x78e1ca58cf10  (unknown)  (unknown)
[tpu-job]    @ 0x41fffffde0b58d4c  (unknown)  (unknown)
[tpu-job]https://symbolize.stripped_domain/r/?trace=790089c1ace1,78fb372056c0,790089c1ad5f,78e1ca58c20f,78e1ca58cf0f,41fffffde0b58d4b&map= 
[tpu-job]E0422 19:26:02.968625    1211 coredump_hook.cc:364] RAW: Remote crash data gathering hook invoked.
[tpu-job]E0422 19:26:02.968632    1211 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
[tpu-job]E0422 19:26:02.968634    1211 coredump_hook.cc:459] RAW: Sending fingerprint to remote end.
[tpu-job]E0422 19:26:02.968648    1211 coredump_hook.cc:468] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
[tpu-job]E0422 19:26:02.968653    1211 coredump_hook.cc:520] RAW: Dumping core locally.
[tpu-job]E0422 19:26:02.992803    1207 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:03.128493    1209 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:03.239222    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:03.685551    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:03.793841    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:03.801738    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:03.968696    1210 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:04.057872    1203 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:04.128803    1366 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:04.143711    1207 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:04.177122    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:04.202808    1209 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:04.634883    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:04.847462    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:04.877496    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:05.060037    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:05.106467    1208 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:05.210625    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:05.398533    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:05.445487    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:05.644101    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:05.663757    1209 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:05.704048    1215 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:05.902284    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:06.141249    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:07.431569    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:07.699104    1207 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:08.031789    1210 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:08.748422    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:08.822786    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:08.893633    1210 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:08.979979    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:09.937339    1207 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:10.059920    1210 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:10.346942    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:10.391024    1207 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:10.610343    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:10.686190    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:10.754518    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:10.821657    1210 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:10.947693    1209 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:11.215211    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:11.420339    1207 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:11.454281    1209 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:11.653183    1207 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:11.664447    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:11.725914    1207 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:11.868609    1207 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:11.967345    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:12.120640    1209 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:12.274470    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:12.434460    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:12.853095    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:13.125308    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]E0422 19:26:13.763816    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:14.509563    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:14.975123    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:15.322684    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:17.674219    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]Training script succeeded
[tpu-job]E0422 19:26:23.107591    1211 process_state.cc:805] RAW: Raising signal 6 with default behavior
[tpu-job]Training script succeeded
