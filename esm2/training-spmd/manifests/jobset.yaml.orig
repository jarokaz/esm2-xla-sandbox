# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: llama2-training  
  namespace: tpu-training
  labels:
    kueue.x-k8s.io/queue-name: tpu-training-jobs
  annotations:
    alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepool # 1:1 job replica to node pool assignment
spec:
  successPolicy:
    operator: All
    targetReplicatedJobs:
    - slice
  failurePolicy:
    maxRestarts: 1  
  replicatedJobs:
    - name: slice    # Part of the name of the child Jobs (<replicateJobName>)
      replicas: 1    # Number of slices
      template:
        spec:
          parallelism: 1   # Must be set to number of nodes in each node pool
          completions: 1   # Must be set to number of nodes in each node pool
          backoffLimit: 0   # Must be set to 0. Fail the job when any pod fails.
          template:
            spec:
              hostNetwork: true
              dnsPolicy: ClusterFirstWithHostNet
              nodeSelector:
                cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice
                cloud.google.com/gke-tpu-topology: 2x2
              containers:
              - name: tpu-job 
                image: pytorch-xla
                ports:
                - containerPort: 8479
                - containerPort: 8478
                - containerPort: 8477
                - containerPort: 8476
                - containerPort: 8431 # Port to export TPU usage metrics, if supported
                securityContext:
                  privileged: true
                command:
                - bash
                - -c
                - |
                  python \
                  examples/pytorch/language-modeling/run_clm.py \
                  --tokenizer_name gpt2 \
                  --dataset_name wikitext \
                  --dataset_config_name wikitext-2-raw-v1 \
                  --per_device_train_batch_size 8 \
                  --per_device_eval_batch_size 8 \
                  --num_train_epochs 2 \
                  --do_train \
                  --output_dir gs://jk101-artifact-repository/runs/101 \
                  --disable_tqdm true \
                  --overwrite_output_dir \
                  --config_name config.json \
                  --save_strategy no \
                  --logging_steps 10 \
                  --logging_strategy steps \
                  --remove_unused_columns no \
                  --spmd_2d_sharding 4 \
                  --torch_dtype bfloat16 \
                  --dataloader_drop_last yes \
                  --spmd_grad_chkpt
                env:
                - name: PJRT_DEVICE
                  value: 'TPU'
                - name: XLA_USE_BF16
                  value: '1'
                resources:
                  limits:
                    google.com/tpu: 4 # Number of Cloud TPU VMs per worker