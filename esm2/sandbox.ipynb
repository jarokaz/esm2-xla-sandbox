{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, RandomSampler\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    SchedulerType,\n",
    "    HfArgumentParser,\n",
    "    is_torch_tpu_available,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bebff43301f4f2aa967f9b341f03895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "        num_rows: 10000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = \"/home/jarekk/datasets/uniref50\"\n",
    "\n",
    "dataset = datasets.load_from_disk(dataset_dir)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041af0ddccf54a409745c3714d086eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0947c033f14af6bf68f05aea1de0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71666ce36804871b9f04851c7e790cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"facebook/esm2_t48_15B_UR50D\"\n",
    "mlm_probability = 0.15\n",
    "\n",
    "tokenizer_kwargs = {}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, **tokenizer_kwargs)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm_probability=mlm_probability\n",
    "    )\n",
    "\n",
    "loader = DataLoader(\n",
    "        dataset[\"train\"],\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_iter = iter(loader)\n",
    "batch = next(loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 20,  4, 19,  7, 32, 11, 16, 32, 17,  9, 15, 19, 20,  6, 13,  5,  4,\n",
       "          4, 16, 15, 11, 19, 11,  7, 32, 32,  4,  8, 15, 32, 10,  7, 15,  0, 16,\n",
       "          6, 12,  7, 14, 32, 19, 19, 12,  9, 32, 17, 21,  9,  5, 12, 32, 14, 10,\n",
       "          9,  4, 18, 32, 10,  5, 16,  9,  9, 15,  5, 32, 10,  5, 32, 12, 19, 10,\n",
       "         10,  8, 11,  9, 15, 15,  8, 15, 32,  9, 32,  8, 15, 19,  8,  8, 15, 19,\n",
       "          8, 32, 29, 13, 12, 20,  7, 23,  6,  9, 23,  6, 16, 14, 19, 10, 10, 16,\n",
       "         11, 32,  8, 32, 19,  6, 16, 15,  8,  5,  7, 22, 10, 23, 13, 17, 10,  4,\n",
       "         32, 17,  6, 11, 15, 17, 23, 15, 32,  8, 14, 32, 32, 15,  9, 16, 14,  4,\n",
       "         19,  9,  5, 32, 20, 11,  5, 12, 17,  8,  7,  7,  9, 17, 16,  6, 32, 18,\n",
       "          7,  6,  5, 18, 10,  9, 25,  7, 12, 10,  7, 12,  6,  8, 19,  8, 11, 10,\n",
       "         21, 12, 14,  8,  9, 19, 32, 32, 16, 12,  9, 15,  4, 16,  6,  9, 20,  4,\n",
       "         11,  4, 12,  9,  9, 17,  5, 15, 16,  6,  8,  7, 17,  9, 17,  8, 13,  9,\n",
       "         16, 19, 21, 32, 12,  5,  9, 10, 12, 32, 13,  4, 15, 32, 15, 15,  4, 15,\n",
       "         32, 11, 32,  9, 32, 32,  4,  5,  9, 11, 18,  8,  9, 10, 12, 17, 13,  7,\n",
       "         32, 21, 23,  4, 15, 13,  8, 11, 23, 32,  7,  6,  6, 19, 13, 13,  9,  4,\n",
       "          7, 32, 17,  4,  4, 16,  8,  7, 15,  7, 12,  8, 32, 13, 15, 12,  9, 12,\n",
       "         16, 18, 15,  8,  6, 12, 32, 28, 16, 16, 10,  7,  5, 19, 13,  9,  2,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100, -100, -100, -100, -100,    4, -100, -100,    5, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,   11,\n",
       "         -100,   13,   18, -100, -100, -100,   15, -100, -100, -100,   17, -100,\n",
       "         -100, -100, -100, -100,   16,   19, -100, -100, -100,   13, -100, -100,\n",
       "         -100, -100, -100,   12, -100, -100, -100, -100, -100,   19, -100, -100,\n",
       "         -100, -100, -100, -100, -100,   10, -100, -100,    5, -100, -100, -100,\n",
       "           14, -100, -100,    9, -100, -100, -100, -100,   11, -100,   15, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100,    4,    8, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100,   22, -100,   15, -100, -100, -100, -100,    8, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100,   15, -100, -100, -100, -100, -100,\n",
       "         -100, -100,   21, -100, -100,   11,    4, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100,   12, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100,    9, -100, -100, -100, -100, -100, -100, -100,\n",
       "           17, -100, -100,   10, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100,   13,    9, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100,   16, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100,   17, -100, -100, -100, -100, -100,   10, -100, -100,\n",
       "         -100,    4, -100, -100, -100, -100,    4, -100,   16, -100,   17,   10,\n",
       "         -100, -100, -100, -100, -100, -100,   16, -100, -100, -100, -100, -100,\n",
       "           13, -100, -100, -100, -100, -100, -100, -100, -100,   11, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100,   10,   10, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100,    9, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100,    7,   20, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch['input_ids']\n",
    "attention_mask = batch['attention_mask']\n",
    "labels = batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e4c7c2876d447f969397d909425615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "config_kwargs = {}\n",
    "config = AutoConfig.from_pretrained(\n",
    "        model_id, **config_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EsmConfig {\n",
       "  \"_name_or_path\": \"facebook/esm2_t48_15B_UR50D\",\n",
       "  \"architectures\": [\n",
       "    \"EsmForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"emb_layer_norm_before\": false,\n",
       "  \"esmfold_config\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 5120,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 20480,\n",
       "  \"is_folding_model\": false,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"mask_token_id\": 32,\n",
       "  \"max_position_embeddings\": 1026,\n",
       "  \"model_type\": \"esm\",\n",
       "  \"num_attention_heads\": 40,\n",
       "  \"num_hidden_layers\": 48,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"rotary\",\n",
       "  \"token_dropout\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.39.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_list\": null,\n",
       "  \"vocab_size\": 33\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_ids.numpy()\n",
    "attention_mask = attention_mask.numpy()\n",
    "labels=labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.device(\"meta\"): \n",
    "    model = AutoModelForMaskedLM.from_config(config)\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    labels = torch.tensor(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmForMaskedLM(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 5120, padding_idx=1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 5120, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-47): 48 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "              (key): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "              (value): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (rotary_embeddings): RotaryEmbedding()\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=1920, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): EsmLMHead(\n",
      "    (dense): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=5120, out_features=33, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flop_counter = FlopCounterMode(model, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module                         FLOP    % Total\n",
      "-----------------------  ----------  ---------\n",
      "EsmForMaskedLM           47239.791B    100.00%\n",
      " - aten.addmm            15488.726B     32.79%\n",
      " - aten.bmm                773.094B      1.64%\n",
      " - aten.mm               30977.971B     65.58%\n",
      " EsmForMaskedLM.esm      47158.741B     99.83%\n",
      "  - aten.addmm           15461.882B     32.73%\n",
      "  - aten.bmm               773.094B      1.64%\n",
      "  - aten.mm              30923.765B     65.46%\n",
      " EsmForMaskedLM.lm_head     81.050B      0.17%\n",
      "  - aten.addmm              26.844B      0.06%\n",
      "  - aten.mm                 54.206B      0.11%\n"
     ]
    }
   ],
   "source": [
    "with flop_counter:\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels\n",
    "    )\n",
    "    outputs.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47239790592000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops_per_sequence = flop_counter.get_total_flops()\n",
    "flops_per_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92265216000.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_flops = flops_per_sequence/512\n",
    "model_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastapi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
